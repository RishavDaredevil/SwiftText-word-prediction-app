anti_join(stop_words) |>
inner_join(get_sentiments("bing")) |>
count(word,sentiment,sort = T) |>
select(-sentiment)   |>
with(wordcloud(words = word,freq = n,max.words = 100))
df_blog |>
unnest_tokens(word,text) |>
anti_join(stop_words) |>
inner_join(get_sentiments("bing")) |>
count(word,sentiment,sort = T) |>
slice_max(n,n = 40) |>
select(-sentiment) |>
wordcloud2(color = "black")
df_twit |>
unnest_tokens(word,text) |>
anti_join(stop_words) |>
inner_join(get_sentiments("bing")) |>
count(word,sentiment,sort = T) |>
slice_max(n,n = 40) |>
select(-sentiment) |>
wordcloud2(color = "black")
df_news |>
unnest_tokens(word,text) |>
anti_join(stop_words) |>
inner_join(get_sentiments("bing")) |>
count(word,sentiment,sort = T) |>
slice_max(n,n = 40) |>
select(-sentiment) |>
wordcloud2(color = "black")
df_news |>
unnest_tokens(word,text) |>
anti_join(stop_words) |>
inner_join(get_sentiments("bing")) |>
count(word,sentiment,sort = T) |>
select(-sentiment) |>
wordcloud2(color = "black")
df_news |>
unnest_tokens(word,text) |>
anti_join(stop_words) |>
inner_join(get_sentiments("bing")) |>
count(word,sentiment,sort = T) |>
slice_max(n,n = 100) |>
select(-sentiment) |>
wordcloud2(color = "black")
df_news |>
unnest_tokens(word,text) |>
anti_join(stop_words) |>
slice_max(n,n = 100) |>
select(-sentiment) |>
wordcloud2(color = "black")
df_news |>
unnest_tokens(word,text) |>
anti_join(stop_words) |>
count(word,sort = T) |>
slice_max(n,n = 100) |>
select(-sentiment) |>
wordcloud2(color = "black")
df_news |>
unnest_tokens(word,text) |>
anti_join(stop_words) |>
count(word,sort = T) |>
slice_max(n,n = 100) |>
wordcloud2(color = "black")
df_news |>
unnest_tokens(word,text) |>
anti_join(stop_words) |>
inner_join(get_sentiments("bing")) |>
count(word,sentiment,sort = T) |>
slice_max(n,n = 40) |>
select(-sentiment) |>
wordcloud2(color = "black")
count_ngrams_tidytable <- function(dataset,n) {
library(tidytable)
coll_name_gram <- case_when(
n == 2 ~ "bigram",
n == 3 ~ "trigram",
.default = "polygram"
)
col_name_df <- furrr::future_map_chr(1:n,~paste0("word",.x),.progress = TRUE)
dataset |>
tidytable(text = _) |>
mutate(id = row_number()) |>
slice_sample(n = 100000) |>
unnest_tokens(!!coll_name_gram, text, token = "ngrams", n = n)|>
separate(eval(coll_name_gram), eval(col_name_df), sep = " ") |>
list() |>
furrr::future_map(~filter(.x,if_all(contains("word"),
~ !.  %in% stop_words$word)) |>
filter(.df = _,if_all(contains("word"),
~ !str_detect(., "\\d"))) ,.progress = TRUE) |>
list_rbind() |>
drop_na() |>
count(contains("word"),sort = T) -> counter
detach("package:tidytable", unload = TRUE)
unloadNamespace("tidytable")
return(counter)
}
twit_bigram <- count_ngrams_tidytable(vec_twit,n = 2)
visualize_ngrams <- function(bigrams) {
set.seed(2016)
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
bigrams  |>
graph_from_data_frame()  |>
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = n, edge_width = n), show.legend = FALSE, arrow = a, edge_colour = "darkred") +
geom_node_point(size = 5) +
geom_node_text(aes(label = name), repel = TRUE,
point.padding = unit(0.2, "lines")) +
theme_void()
}
twit_bigram |>
filter(n > 25,
!str_detect(word1, "\\d"),
!str_detect(word2, "\\d")) |>
visualize_ngrams() + ggtitle("bigram")
library(ggraph)
library(igraph)
library(tidytext)
library(furrr)
twit_trigram |>
filter(n>5,!str_detect(word1, "\\d"),
!str_detect(word2, "\\d"),!str_detect(word3, "\\d")) |>
visualize_ngrams() +  ggtitle("Trigram n>5") -> temp
twit_bigram |>
filter(n > 25,
!str_detect(word1, "\\d"),
!str_detect(word2, "\\d")) |>
visualize_ngrams() + ggtitle("bigram")
visualize_ngrams <- function(bigrams) {
set.seed(2016)
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
bigrams  |>
graph_from_data_frame()  |>
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = n, edge_width = n), show.legend = FALSE, arrow = a, edge_colour = "darkred") +
geom_node_point(size = 5,alpha = .5) +
geom_node_text(aes(label = name), repel = TRUE,
point.padding = unit(0.2, "lines")) +
theme_void()
}
twit_bigram |>
filter(n > 25,
!str_detect(word1, "\\d"),
!str_detect(word2, "\\d")) |>
visualize_ngrams() + ggtitle("bigram")
combi_df_bi <- count_ngrams_tidytable(combi_vec,2)
combi_df_bi_lab |> as_tibble() |>  filter(!str_detect(word1, "\\d"),!str_detect(word2, "\\d")) |>
unite(bigram,word1,word2,sep = " ") |> group_by(type) |> slice_max(n,n = 15) |> ungroup() |>
ggplot(aes(reorder(bigram,n),n,fill = type)) + geom_col() + coord_flip()+ ggtitle("Word Freq bigram Top 10 from each Corpus")+
xlab("word") + facet_wrap(~type,scales = "free")
combi_df_bi |> as_tibble() |>  filter(!str_detect(word1, "\\d"),!str_detect(word2, "\\d")) |>
unite(bigram,word1,word2,sep = " ") |> group_by(type) |> slice_max(n,n = 15) |> ungroup() |>
ggplot(aes(reorder(bigram,n),n,fill = type)) + geom_col() + coord_flip()+ ggtitle("Word Freq bigram Top 10 from each Corpus")+
xlab("word") + facet_wrap(~type,scales = "free")
combi_df_tri <- count_ngrams_tidytable(combi_vec,3)
combi_df_tri |> as_tibble() |>  filter(n>7,!str_detect(word1, "\\d"),
!str_detect(word2, "\\d"),
!str_detect(word3, "\\d")) |>
unite(trigram,word1,word2,word3,sep = " ") |>
ggplot(aes(reorder(trigram,n),n)) + geom_col() +
coord_flip()+ ggtitle("word freq trigram n > 7")+
xlab("word")
combi_df_tri_lab |> as_tibble() |>  filter(!str_detect(word1, "\\d"),
!str_detect(word2, "\\d"),
!str_detect(word3, "\\d")) |>
unite(trigram,word1,word2,word3,sep = " ") |> group_by(type) |> slice_max(n,n = 15) |>
ggplot(aes(reorder(trigram,n),n,fill = type)) + geom_col() +
coord_flip()+ ggtitle("Word Freq trigram Top 10 from each Corpus")+
xlab("word") + facet_wrap(~type,scales = "free")
visualize_ngrams <- function(bigrams) {
set.seed(2016)
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
bigrams  |>
graph_from_data_frame()  |>
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, arrow = a, edge_colour = "darkred") +
geom_node_point(size = 5,alpha = .5) +
geom_node_text(aes(label = name), repel = TRUE,
point.padding = unit(0.2, "lines")) +
theme_void()
}
twit_bigram |>
filter(n > 25,
!str_detect(word1, "\\d"),
!str_detect(word2, "\\d")) |>
visualize_ngrams() + ggtitle("bigram")
knitr::opts_chunk$set(echo = FALSE,warning = FALSE,message = FALSE)
size_of_files <- sapply(all_samp_df,object_size)
all_samp_df <- list(vec_twit = vec_twit,vec_blog = vec_blog,vec_news = vec_news)
vec_twit <- read_lines(list.files(path = path,pattern = "twitter",full.names = T),lazy = T)
vec_blog <- read_lines(list.files(path = path,pattern = "blog",full.names = T),lazy = T)
vec_news <- read_lines(list.files(path = path,pattern = "news",full.names = T),lazy = T)
path <- ("C:/Users/Risha/Desktop/R_course/Captstone/Coursera-SwiftKey/final/en_US")
vec_twit <- read_lines(list.files(path = path,pattern = "twitter",full.names = T),lazy = T)
size_of_files <- sapply(all_samp_df,object_size)
all_samp_df <- list(vec_twit = vec_twit,vec_blog = vec_blog,vec_news = vec_news)
vec_blog <- read_lines(list.files(path = path,pattern = "blog",full.names = T),lazy = T)
all_samp_df <- list(vec_twit = vec_twit,vec_blog = vec_blog,vec_news = vec_news)
vec_news <- read_lines(list.files(path = path,pattern = "news",full.names = T),lazy = T)
all_samp_df <- list(vec_twit = vec_twit,vec_blog = vec_blog,vec_news = vec_news)
size_of_files <- sapply(all_samp_df,object_size)
size_of_files
vec_twit <- read_lines(list.files(path = path,pattern = "twitter",full.names = T),lazy = T)
vec_blog <- read_lines(list.files(path = path,pattern = "blog",full.names = T),lazy = T)
vec_news <- read_lines(list.files(path = path,pattern = "news",full.names = T),lazy = T)
object_size(vec_blog)
vec_twit <- read_lines(list.files(path = path,pattern = "twitter",full.names = T))
vec_twit <- read_lines(list.files(path = path,pattern = "twitter",full.names = T))
vec_blog <- read_lines(list.files(path = path,pattern = "blog",full.names = T))
vec_blog <- read_lines(list.files(path = path,pattern = "blog",full.names = T))
vec_news <- read_lines(list.files(path = path,pattern = "news",full.names = T))
all_samp_df <- list(vec_twit = vec_twit,vec_blog = vec_blog,vec_news = vec_news)
size_of_files <- sapply(all_samp_df,object_size)
size_of_files
object_size(vec_blog)
size_of_files
object_size(vec_twit)
object_size(vec_news)
setwd(("C:/Users/Risha/Desktop/R_course/Captstone/Coursera-SwiftKey/final/en_US"))
remove.packages("rsconnect") #Remove Installed Packages
remotes::install_version("rsconnect", version = "0.8.29") #Installing a Specific Version of a Package
rsconnect::rpubsUpload("Milestone Report
","Milestone-Report
",id = 1110796)
rsconnect::rpubsUpload("Milestone Report
","Milestone-Report.html
",id = 1110796)
rlang::last_trace()
rlang::last_trace(drop = FALSE)
rsconnect::rpubsUpload("Milestone Report
","Milestone-Report",id = 1110796)
rsconnect::rpubsUpload("Milestone Report
","Milestone-Report.html",id = 1110796)
setwd("C:/Users/Risha/Desktop/R_course/Captstone")
rsconnect::rpubsUpload("Milestone Report
","Milestone-Report.html",id = 1110796)
rsconnect::rpubsUpload("Milestone Report
","Milestone-Report",id = 1110796)
rsconnect::rpubsUpload("Milestone Report
","Milestone-Report.html",id = 1110796)
packageVersion("rsconnect")
install.packages(c("evaluate", "fBasics", "graphlayouts", "htmltools", "lava", "lifecycle", "lme4", "maps", "Matrix", "MatrixModels", "matrixStats", "parameters", "plotrix", "pROC", "RcppEigen", "RCurl", "Rdpack", "Rfast", "rlang", "rprojroot", "rsconnect", "text2vec", "waldo", "warp", "xfun", "XML"))
install.packages(c("evaluate", "fBasics", "graphlayouts", "htmltools", "lava", "lifecycle", "lme4", "maps", "Matrix", "MatrixModels", "matrixStats", "parameters", "plotrix", "pROC", "RcppEigen", "RCurl", "Rdpack", "Rfast", "rlang", "rprojroot", "rsconnect", "text2vec", "waldo", "warp", "xfun", "XML"))
install.packages("bootstrap")
bootstrap::cell
count_ngrams_tidytable <- function(dataset,n) {
library(tidytable)
coll_name_gram <- case_when(
n == 2 ~ "bigram",
n == 3 ~ "trigram",
.default = "polygram"
)
col_name_df <- furrr::future_map_chr(1:n,~paste0("word",.x),.progress = TRUE)
dataset |>
tidytable(text = _) |>
mutate(id = row_number()) |>
slice_sample(n = 100000) |>
unnest_tokens(!!coll_name_gram, text, token = "ngrams", n = n)|>
separate(!!coll_name_gram, !!col_name_df, sep = " ") |>
list() |>
furrr::future_map(~filter(.x,if_all(contains("word"),
~ !.  %in% stop_words$word)) |>
filter(.df = _,if_all(contains("word"),
~ !str_detect(., "\\d"))) ,.progress = TRUE) |>
list_rbind() |>
drop_na() |>
count(contains("word"),sort = T) -> counter
detach("package:tidytable", unload = TRUE)
unloadNamespace("tidytable")
return(counter)
}
library(tidytext)
library(widyr)
path <- ("C:/Users/Risha/Desktop/R_course/Captstone/Coursera-SwiftKey/final/en_US")
vec_twit <- read_lines(list.files(path = path,pattern = "twitter",full.names = T),lazy = T)
loadingNamespaceInfo()
library(tidyverse)
library(tidytext)
library(widyr)
path <- ("C:/Users/Risha/Desktop/R_course/Captstone/Coursera-SwiftKey/final/en_US")
vec_twit <- read_lines(list.files(path = path,pattern = "twitter",full.names = T),lazy = T)
vec_blog <- read_lines(list.files(path = path,pattern = "blog",full.names = T),lazy = T)
vec_news <- read_lines(list.files(path = path,pattern = "news",full.names = T),lazy = T)
combi_vec <- c(vec_twit,vec_blog,vec_news)
# TEST
twit_trigram <- count_ngrams_tidytable(dataset = vec_twit,n = 3)
count_ngrams_tidytable <- function(dataset,n) {
library(tidytable)
coll_name_gram <- case_when(
n == 2 ~ "bigram",
n == 3 ~ "trigram",
.default = "polygram"
)
col_name_df <- furrr::future_map_chr(1:n,~paste0("word",.x),.progress = TRUE)
dataset |>
tidytable(text = _) |>
mutate(id = row_number()) |>
slice_sample(n = 100000) |>
unnest_tokens(!!coll_name_gram, text, token = "ngrams", n = n)|>
separate(!!coll_name_gram,col_name_df, sep = " ") |>
list() |>
furrr::future_map(~filter(.x,if_all(contains("word"),
~ !.  %in% stop_words$word)) |>
filter(.df = _,if_all(contains("word"),
~ !str_detect(., "\\d"))) ,.progress = TRUE) |>
list_rbind() |>
drop_na() |>
count(contains("word"),sort = T) -> counter
detach("package:tidytable", unload = TRUE)
unloadNamespace("tidytable")
return(counter)
}
# TEST
twit_trigram <- count_ngrams_tidytable(dataset = vec_twit,n = 3)
twit_trigram
abg <- "abg"
!!abg
!!expr(abg)
!!sym(abg)
sym(abg)
!!sym(abg)
expr(abg)
z <- expr(abg)
z
!!z
eval(z)
!!z
!!!z
!z
sym(z)
!!sym(z)
enquo(abg)
enquo(abg)
!!enquo(abg)
!!abg
!!as_label(z)
!!as_label(abg)
!!as_label(enquo(abg))
as_label(enquo(abg))
as_label(enquo(z))
!!enquo(z)
enquo(z)
as_label(enquo(z))
as_name(z)
as_name(abg)
setwd("C:/Users/Risha/Desktop/R_course/Developing_Data_Products")
![Performance Metrics](images/stats.png){width="701"}
# Exploratory Analysis of the Corpus
## Introduction
This documentation describes the exploratory analysis and data preprocessing performed in response to the project's requirements. The analysis is part of an ongoing project aimed at developing a prediction algorithm and a Shiny app. The main goal here is to demonstrate an understanding of the data and to summarize the initial findings concisely for a non-data scientist manager.
knitr::opts_chunk$set(echo = TRUE,warning = FALSE,message = FALSE)
library(tidyverse)
library(tidytext)
library(widyr)
library(patchwork)
library(pryr)
library(gt)
path <- ("C:/Users/Risha/Desktop/R_course/Captstone/Coursera-SwiftKey/final/en_US")
vec_twit <- read_lines(list.files(path = path,pattern = "twitter",full.names = T),lazy = T)
vec_blog <- read_lines(list.files(path = path,pattern = "blog",full.names = T),lazy = T)
vec_news <- read_lines(list.files(path = path,pattern = "news",full.names = T),lazy = T)
all_samp_df <- list(vec_twit = vec_twit,vec_blog = vec_blog,vec_news = vec_news)
size_of_files <- c(334.48,267.76,269.84)
map_dbl(all_samp_df,~length(.x)) |> tibble(no.of.lines = _) |>
mutate(names_of_files = names(all_samp_df),size_of_files_in_MB = size_of_files) |>
relocate(names_of_files,size_of_files_in_MB)|>
gt()
add_rn <- as_mapper(~tibble(text = .x) |>
mutate(id = row_number()) |>
slice_sample(n = 10000))
df_twit <- add_rn(vec_twit)
df_blog <- add_rn(vec_blog)
df_news <- add_rn(vec_news)
frequency <- bind_rows(mutate(df_twit, type = "twitter"),
mutate(df_blog, type = "blogs"),
mutate(df_news, type = "news")) |>
unnest_tokens(word,text) |>
anti_join(stop_words) |>
filter(!str_detect(word, "\\d")) |>
count(type, word) |>
group_by(type) |>
mutate(proportion = n / sum(n)) |>
select(-n) |>
pivot_wider(names_from = type, values_from = proportion) |>
pivot_longer(blogs:news,
names_to = "type", values_to = "proportion")
add_rn <- as_mapper(~tibble(text = .x) |>
mutate(id = row_number()) |>
slice_sample(n = 10000))
df_twit <- add_rn(vec_twit)
df_blog <- add_rn(vec_blog)
df_news <- add_rn(vec_news)
frequency <- bind_rows(mutate(df_twit, type = "twitter"),
mutate(df_blog, type = "blogs"),
mutate(df_news, type = "news")) |>
unnest_tokens(word,text) |>
anti_join(stop_words) |>
filter(!str_detect(word, "\\d")) |>
count(type, word) |>
group_by(type) |>
mutate(proportion = n / sum(n)) |>
select(-n)
pivot_wider(names_from = type, values_from = proportion) |>
pivot_longer(blogs:news,
names_to = "type", values_to = "proportion")
frequency <- bind_rows(mutate(df_twit, type = "twitter"),
mutate(df_blog, type = "blogs"),
mutate(df_news, type = "news")) |>
unnest_tokens(word,text) |>
anti_join(stop_words) |>
filter(!str_detect(word, "\\d")) |>
count(type, word) |>
group_by(type) |>
mutate(proportion = n / sum(n)) |>
select(-n)
bind_rows(mutate(df_twit, type = "twitter"),
mutate(df_blog, type = "blogs"),
mutate(df_news, type = "news")) |>
unnest_tokens(word,text) |>
anti_join(stop_words) |>
filter(!str_detect(word, "\\d")) |>
count(type, word) |>
group_by(type) |>
mutate(proportion = n / sum(n)) |>
select(-n)
frequency <- bind_rows(mutate(df_twit, type = "twitter"),
mutate(df_blog, type = "blogs"),
mutate(df_news, type = "news")) |>
unnest_tokens(word,text) |>
anti_join(stop_words) |>
filter(!str_detect(word, "\\d")) |>
count(type, word) |>
group_by(type) |>
mutate(proportion = n / sum(n)) |>
select(-n) |> print()
bind_rows(mutate(df_twit, type = "twitter"),
mutate(df_blog, type = "blogs"),
mutate(df_news, type = "news")) |>
unnest_tokens(word,text) |>
anti_join(stop_words) |>
filter(!str_detect(word, "\\d")) |>
count(type, word) |>
group_by(type) |>
mutate(proportion = n / sum(n)) |>
select(-n) |> print()
bind_rows(mutate(df_twit, type = "twitter"),
mutate(df_blog, type = "blogs"),
mutate(df_news, type = "news")) |>
unnest_tokens(word,text) |>
anti_join(stop_words) |>
filter(!str_detect(word, "\\d")) |>
count(type, word) |>
group_by(type) |>
mutate(proportion = n / sum(n)) |>
select(-n)
df_twit
setwd("C:/Users/Risha/Desktop/R_course/Captstone/Coursera_prediction_proj/Generating_n-grams_data")
library(profvis)
source("helper.r")
source("helper.r")
predicting("how are you")
profvis({predicting("how are you")})
predicting <- function(x){
predictions <- character(length = 3)
dist_closest <<- split_clean_usr_txt(x)
# for 4 gram
if (length(dist_closest)>=3 ) {
case4()-> predictions
}
# for tri gram
else if (length(dist_closest)==2 ) {
case3()-> predictions
}
# for bi gram
else if (length(dist_closest)==1 ) {
case2()-> predictions
}
# for uni gram
else {
case1()-> predictions
}
return(predictions)
}
profvis({predicting("how are you")})
predicting("how are you")
predicting("how are you")
predicting("how are you")
predicting("how are you")
predicting("how are you")
predicting("how are you")
predicting("how are you")
predicting("how are you")
install.packages("hunspell")
library(hunspell)
# Function to suggest corrections for misspelled words
suggest_corrections <- function(word) {
suggestions <- hunspell_suggest(word)
if (length(suggestions) > 0) {
cat("Did you mean:", suggestions, "\n")
} else {
cat("No suggestions available.\n")
}
}
# Example usage
user_input <- "incorret"
suggest_corrections(user_input)
hunspell_suggest("incorret")
hunspell_suggest(split_clean_usr_txt("hw aree youu"))
find_closest_match(split_clean_usr_txt("hw aree youu"))
map(split_clean_usr_txt("hw aree youu"),~find_closest_match(.x))
library(rsconnect)
deployApp()
